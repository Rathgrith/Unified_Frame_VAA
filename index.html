<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Primary Meta Tags -->
  <meta name="title"
    content="[NeurIPS 2025] A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis">
  <meta name="description"
    content="Test-time reasoning over frozen vision-language models unifies detection, localization, and explanation for zero-shot video anomaly analysis across benchmarks.">
  <meta name="keywords"
    content="zero-shot learning, vision-language models, multimodal reasoning, test-time adaptation, surveillance analytics">
  <meta name="author" content="Dongheng Lin, Mengxue Qu, Kunyang Han, Jianbo Jiao, Xiaojie Jin, Yunchao Wei">
  <meta name="robots" content="noindex, nofollow, noarchive, nosnippet, noimageindex">
  <meta name="googlebot" content="noindex, nofollow, noarchive, nosnippet, noimageindex">
  <meta name="bingbot" content="noindex, nofollow, noarchive, nosnippet, noimageindex">
  <meta name="language" content="English">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis">
  <meta name="citation_author" content="Lin, Dongheng">
  <meta name="citation_author" content="Qu, Mengxue">
  <meta name="citation_author" content="Han, Kunyang">
  <meta name="citation_author" content="Jiao, Jianbo">
  <meta name="citation_author" content="Jin, Xiaojie">
  <meta name="citation_author" content="Wei, Yunchao">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="Advances in Neural Information Processing Systems (NeurIPS) 2025">
  <!-- <meta name="citation_pdf_url" content="https://rathgrith.github.io/Unified_Frame_VAA/_Dongheng_TTR_VAD__NeurIPS_2025.pdf"> -->

  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">

  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>URF-ZS-HVAA
  </title>

  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">

  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">

  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>

  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis",
    "description": "Test-time reasoning over frozen vision-language models unifies detection, localization, and explanation for zero-shot video anomaly analysis across benchmarks.",
    "author": [
      {
        "@type": "Person",
        "name": "Dongheng Lin",
        "affiliation": {
          "@type": "Organization",
          "name": "The MIx Group, University of Birmingham"
        }
      },
      {
        "@type": "Person",
        "name": "Mengxue Qu",
        "affiliation": {
          "@type": "Organization",
          "name": "Institute of Information Science, Beijing Jiaotong University"
        }
      },
      {
        "@type": "Person",
        "name": "Kunyang Han",
        "affiliation": {
          "@type": "Organization",
          "name": "Institute of Information Science, Beijing Jiaotong University"
        }
      },
      {
        "@type": "Person",
        "name": "Jianbo Jiao",
        "affiliation": {
          "@type": "Organization",
          "name": "The MIx Group, University of Birmingham"
        }
      },
      {
        "@type": "Person",
        "name": "Xiaojie Jin",
        "affiliation": {
          "@type": "Organization",
          "name": "Institute of Information Science, Beijing Jiaotong University"
        }
      },
      {
        "@type": "Person",
        "name": "Yunchao Wei",
        "affiliation": {
          "@type": "Organization",
          "name": "Institute of Information Science, Beijing Jiaotong University"
        }
      }
    ],
    "datePublished": "2025-12-01",
    "publisher": {
      "@type": "Organization",
      "name": "Advances in Neural Information Processing Systems"
    },
    "url": "https://rathgrith.github.io/Unified_Frame_VAA/",
    "image": "https://rathgrith.github.io/Unified_Frame_VAA/static/images/social_preview.png",
    "keywords": ["zero-shot learning", "vision-language models", "multimodal reasoning"],
    "abstract": "Most video-anomaly research stops at frame-wise detection, offering little insight into why an event is abnormal, typically outputting only frame-wise anomaly scores without spatial or semantic context. Recent video anomaly localization and video anomaly understanding methods improve explainability but remain data-dependent and task-specific. We propose a unified reasoning framework that bridges the gap between temporal detection, spatial localization, and textual explanation. Our approach is built upon a chained test-time reasoning process that sequentially connects these tasks, enabling holistic zero-shot anomaly analysis without any additional training. Specifically, our approach leverages intra-task reasoning to refine temporal detections and inter-task chaining for spatial and semantic understanding, yielding improved interpretability and generalization in a fully zero-shot manner. Without any additional data or gradients, our method achieves state-of-the-art zero-shot performance across multiple video anomaly detection, localization, and explanation benchmarks. The results demonstrate that careful prompt design with task-wise chaining can unlock the reasoning power of foundation models, enabling practical, interpretable video anomaly analysis in a fully zero-shot manner.",
    "citation": "Lin, D., Qu, M., Han, K., Jiao, J., Jin, X., & Wei, Y. (2025). A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis. Advances in Neural Information Processing Systems.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://rathgrith.github.io/Unified_Frame_VAA/"
    },
    "about": [
      {
        "@type": "Thing", 
        "name": "Vision-language reasoning"
      }
    ]
  }
  </script>

  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Institute of Information Science, Beijing Jiaotong University",
    "url": "https://www.bjtu.edu.cn/",
    "logo": "https://rathgrith.github.io/Unified_Frame_VAA/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/rathgrith",
      "https://github.com/rathgrith"
    ]
  }
  </script>
</head>

<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>


  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">A Unified Reasoning Framework for Holistic Zero-Shot Video
                Anomaly Analysis</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block"><a href="https://rathgrith.github.io/" target="_blank">Dongheng
                    Lin<sup>1,2</sup></a></span>
                <span class="author-block"><a href="https://scholar.google.com/citations?hl=en&user=k8h4bqMAAAAJ"
                    target="_blank">Mengxue Qu<sup>1</sup></a></span>
                <span class="author-block"><a href="https://scholar.google.com/citations?user=ONilnp8AAAAJ"
                    target="_blank">Kunyang Han<sup>1</sup></a></span>
                <span class="author-block"><a href="https://jianbojiao.com/" target="_blank">Jianbo
                    Jiao<sup>2</sup></a></span>
                <span class="author-block"><a href="https://faculty.bjtu.edu.cn/10327/" target="_blank">Xiaojie
                    Jin<sup>1</sup></a></span>
                <span class="author-block"><a href="https://weiyc.github.io/" target="_blank">Yunchao
                    Wei<sup>1</sup></a></span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup>1</sup><a href="http://mepro.bjtu.edu.cn/" target="_blank">Institute of
                    Information Science, BJTU</a></span>
                <span class="author-block"><sup>2</sup><a href="https://mix.jianbojiao.com/" target="_blank">The MIx
                    Group, University of Birmingham</a></span><br>
                <span class="author-block">NeurIPS 2025</span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://openreview.net/pdf?id=Qla5PqFL0s" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/Rathgrith/URF-ZS-HVAA" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2511.00962" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="#BibTeX" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-quote-right"></i>
                      </span>
                      <span>BibTeX</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      </div>
    </section>


    <!-- Teaser video-->
    <!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata">
        <source src="./static/videos/banner_video.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        System overview demo (placeholder). The chained reasoning pipeline links temporal detection, spatial localization, and textual explanation without any finetuning.
      </h2>
    </div>
  </div>
</section> -->
    <!-- End teaser video -->

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              Most video-anomaly research stops at frame-wise detection, offering little insight into why an event is
              abnormal, typically outputting only frame-wise anomaly scores without spatial or semantic context. Recent
              video anomaly localization and video anomaly understanding methods improve explainability but remain
              data-dependent and task-specific. We propose a unified reasoning framework that bridges the gap between
              temporal detection, spatial localization, and textual explanation. Our approach is built upon a chained
              test-time reasoning process that sequentially connects these tasks, enabling holistic zero-shot anomaly
              analysis without any additional training. Specifically, our approach leverages intra-task reasoning to
              refine temporal detections and inter-task chaining for spatial and semantic understanding, yielding
              improved interpretability and generalization in a fully zero-shot manner. Without any additional data or
              gradients, our method achieves state-of-the-art zero-shot performance across multiple video anomaly
              detection, localization, and explanation benchmarks. The results demonstrate that careful prompt design
              with task-wise chaining can unlock the reasoning power of foundation models, enabling practical,
              interpretable video anomaly analysis in a fully zero-shot manner.
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->
    <!-- 5-MIN TEASER VIDEO -->

    <!-- Highlights -->

    <section class="section" id="overview">
      <div class="container is-max-desktop">
        <h2 class="title is-3">Highlights</h2>
        <div class="content">
          <p>
            We present a unified reasoning framework that connects temporal detection, spatial localization, and textual
            explanation for holistic video anomaly analysis in a fully zero-shot manner. Our key contributions include:
          </p>

          <image src="./static/images/figure1.png" alt="Overview of the unified reasoning framework" loading="lazy" />
          <ul>
            <li>Introduces a unified, training-free pipeline that chains temporal detection, spatial localization, and
              textual explanation for holistic video anomaly analysis.</li>
            <li>Proposes Intra-Task Reasoning (IntraTR) to refine temporal anomaly scores using contextual video priors,
              enhancing detection accuracy.</li>
            <li>Develops Inter-Task Chaining (InterTC) to link temporal detection with spatial and semantic tasks,
              guiding frozen detectors and narrators using temporal cues.</li>
            <li>Establishes new state-of-the-art zero-shot baselines across multiple video anomaly detection,
              localization, and explanation benchmarks without any additional training or data.</li>
          </ul>
        </div>
      </div>
    </section>
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <h2 class="title is-3">Video Presentation</h2>
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="publication-video placeholder-video">
                <video poster="" id="teaser_video" controls muted loop height="100%" preload="metadata">
                  <source src="./static/videos/teaser.mp4" type="video/mp4">
                </video>
              </div>
              <h2 class="subtitle has-text-centered mt-4">

              </h2>
            </div>
          </div>
        </div>
      </div>
      <section class="section" id="method">
        <div class="container is-max-desktop">
          <h2 class="title is-3">Methodology</h2>
          The proposed framework consists of two key components: <strong>Intra-Task Reasoning (IntraTR)</strong> and
          <strong>Inter-Task Chaining
            (InterTC)</strong>. IntraTR refines temporal anomaly scores by leveraging priors from the most suspicious
          video segment,
          while InterTC connects temporal detection with spatial localization and textual explanation, guiding frozen
          detectors and narrators using temporal cues. This chained reasoning process enables comprehensive zero-shot
          video anomaly analysis without any additional training or data.
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <img src="./static/images/intraTR.png" alt="Intra-Task Reasoning methodology" loading="lazy"
                  style="width: 50%; margin: 0 auto; display: block;" />
                <h2 class="subtitle has-text-centered">
                  Intra-Task Reasoning (IntraTR) enhances temporal anomaly detection by refining scores based on
                  contextual video segments.
                </h2>
              </div>
              <div class="item">
                <img src="./static/images/interTC.png" alt="Inter-Task Chaining methodology" loading="lazy"
                  style="width: 40%; margin: 0 auto; display: block;" />
                <h2 class="subtitle has-text-centered">
                  Inter-Task Chaining (InterTC) links temporal detection with spatial localization and textual
                  explanation, enabling holistic anomaly analysis.
                </h2>
              </div>
            </div>
          </div>
        </div>
      </section>


      <!-- <section class="section" id="results">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Results at a Glance</h2>
    <div class="content">
      <p>
        The unified reasoning framework establishes new zero-shot baselines across four video anomaly benchmarks while remaining completely training-free. Below we summarize the key quantitative improvements.
      </p>
      <div class="columns is-centered">
        <div class="column is-10">
          <div id="quantitative-carousel" class="carousel quantitative-carousel">
            <div class="item has-text-centered">
              <h3 class="title is-4">Temporal Video Anomaly Detection (VAD)</h3>
              <figure class="image">
                <img src="static/images/vad_table.png" alt="Temporal VAD results on UCF-Crime and XD-Violence" loading="lazy"/>
              </figure>
              <p class="mt-4">
                Intra-Task Reasoning consistently boosts AUC across backbones and datasets, with adaptive margins providing further gains. Our method outperforms prior zero-shot approaches by a significant margin, closing the gap to supervised methods without any training.
              </p>
            </div>
            <div class="item has-text-centered">
              <h3 class="title is-4">Spatial Video Anomaly Localization (VAL)</h3>
              <figure class="image">
                <img src="static/images/val_table.png" alt="Spatial VAL results on UCF-Crime" loading="lazy"/>
              </figure>
              <p class="mt-4">
                Inter-Task Chaining leverages temporal priors to guide frozen detectors, improving TIoU by +1.1 over direct VLM localization.
              </p>
            </div>
            <div class="item has-text-centered">
              <h3 class="title is-4">Textual Video Anomaly Understanding (VAU)</h3>
              <figure class="image">
                <img src="static/images/vau_table.png" alt="Video anomaly understanding results on UCF-Crime and XD-Violence" loading="lazy"/>
              </figure>
              <p class="mt-4">
                Inter-task chaining consistently improves both traditional and LLM-based metrics over frozen narrators, closing the gap to supervised instruction-tuned systems.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->


      <!-- Image carousel -->
      <section class="hero is-small">
        <div class="hero-body">
          <div class="container is-max-desktop">
            <h2 class="title is-3">Results at a Glance</h2>
            <p>
              The unified reasoning framework establishes new zero-shot baselines across four video anomaly benchmarks
              while remaining completely training-free. Below we summarize the key quantitative improvements.
            </p>
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item has-text-centered">
                <h3 class="title is-4">Temporal Video Anomaly Detection (VAD)</h3>
                <figure class="image"></figure>
                <img src="./static/images/vad_table.png" alt="Temporal VAD results on UCF-Crime and XD-Violence"
                  loading="lazy" style="width: 75%; margin: 0 auto; display: block;" />
                </figure>
                <p class="mt-4">
                  Intra-Task Reasoning consistently boosts AUC across backbones and datasets, with adaptive margins
                  providing further gains. Our method outperforms prior zero-shot approaches by a significant margin,
                  closing the gap to supervised methods without any training.
                </p>
              </div>
              <div class="item has-text-centered">
                <h3 class="title is-4">Spatial Video Anomaly Localization (VAL)</h3>
                <figure class="image">
                  <!-- resize images to 40% and center -->
                  <img src="./static/images/val_table.png" alt="Spatial VAL results on UCF-Crime" loading="lazy"
                    style="width: 30%; margin: 0 auto; display: block; " />
                </figure>
                <p class="mt-4">
                  Inter-Task Chaining leverages temporal priors to guide frozen detectors, improving TIoU by +1.1 over
                  direct VLM localization.
                </p>
                <h3 class="title is-4">Textual Video Anomaly Understanding (VAU)</h3>
                <figure class="image">
                  <img src="./static/images/vau_table.png"
                    alt="Video anomaly understanding results on UCF-Crime and XD-Violence" loading="lazy" />
                </figure>
                <p class="mt-4">
                  Inter-task chaining consistently improves both traditional and LLM-based metrics over frozen
                  narrators,
                  closing the gap to supervised instruction-tuned systems.
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>
      <!-- End image carousel -->




      <!-- Youtube video
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video placeholder-video">
            <p>Video walkthrough coming soon. We will showcase the chained reasoning pipeline and real-world case studies here.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
      <!-- End youtube video -->


      <!-- Video carousel -->
      <section class="hero is-small">
        <div class="hero-body">
          <div class="container is-max-desktop">
            <h2 class="title is-3">Qualitative Results</h2>
            <p>
              Below we present qualitative results demonstrating our method's ability to accurately detect, localize,
              and
              explain anomalies in a fully zero-shot manner across diverse scenarios. In all cases, our method
              effectively
              identifies anomalous events, precisely localizes them in space and time, and generates fine-grained
              anomaly
              tags and coherent textual
              explanations, showcasing the power of chained reasoning over frozen vision-language models.
            </p>
            <!-- one VAL image qualitative visualisation -->
            <p class="has-text-centered"></p>
            <div id="qualitative-carousel" class="carousel results-carousel">
              <div class="item item-video1">
                <!-- Poster image placeholder -->
                <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
                  <!-- Video placeholder -->
                  <source src="./static/videos/qualitative_vad_vau/Arrest001_x264_viz.mp4" type="video/mp4">
                </video>
              </div>
              <div class="item item-video2">
                <!-- Poster image placeholder -->
                <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
                  <!-- Video placeholder -->
                  <source src="./static/videos/qualitative_vad_vau/Assault006_x264_viz.mp4" type="video/mp4">
                </video>
              </div>
              <div class="item item-video3">
                <!-- Poster image placeholder -->
                <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
                  <!-- Video placeholder -->
                  <source src="./static/videos/qualitative_vad_vau/Burglary033_x264_viz.mp4" type="video/mp4">
                </video>
              </div>
              <div class="item item-video4">
                <!-- Poster image placeholder -->
                <video poster="" id="video4" controls muted loop height="100%" preload="metadata">
                  <!-- Video placeholder -->
                  <source src="./static/videos/qualitative_vad_vau/Explosion008_x264_viz.mp4" type="video/mp4">
                </video>
              </div>
              <div class="item item-video5">
                <!-- Poster image placeholder -->
                <video poster="" id="video5" controls muted loop height="100%" preload="metadata">
                  <!-- Video placeholder -->
                  <source src="./static/videos/qualitative_vad_vau/Fighting047_x264_viz.mp4" type="video/mp4">
                </video>
              </div>
              <div class="item item-video6">
                <!-- Poster image placeholder -->
                <video poster="" id="video6" controls muted loop height="100%" preload="metadata">
                  <!-- Video placeholder -->
                  <source src="./static/videos/qualitative_vad_vau/Robbery102_x264_viz.mp4" type="video/mp4">
                </video>
              </div>
              <div class="item item-video7">
                <!-- Poster image placeholder -->
                <video poster="" id="video7" controls muted loop height="100%" preload="metadata">
                  <!-- Video placeholder -->
                  <source src="./static/videos/qualitative_vad_vau/Normal_Videos_913_x264_viz.mp4" type="video/mp4">
                </video>
              </div>
              <div class="item item-video8">
                <!-- Poster image placeholder -->
                <video poster="" id="video8" controls muted loop height="100%" preload="metadata">
                  <!-- Video placeholder -->
                  <source src="./static/videos/qualitative_vad_vau/Normal_Videos_936_x264_viz.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <p class="mt-4">
              <strong>Qualitative temporal detection and textual explanation results.</strong> Our method accurately
              identifies anomalous
              segments and generates detailed explanations by leveraging chained reasoning across tasks.
            </p>
            <div class="image">
              <img src="static/images/val_qual.png" alt="Qualitative Localisation results on UCF-Crime" loading="lazy" ,
                style="width: 70%; margin: 0 auto; display: block;" />
            </div>
            <p class="mt-4">
              <strong>Qualitative spatial localization results on UCF-Crime.</strong> Our method accurately localizes
              anomalies by
              leveraging extracted priors from temporal task to guide frozen VLM detectors.
            </p>
          </div>

        </div>
      </section>
      <!-- End video carousel -->






      <!-- Paper poster -->
      <!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <div class="content has-text-centered">
        <p>Poster coming soon. We will share a condensed visual summary of IntraTR and InterTC once finalized.</p>
      </div>
      </div>
    </div>
  </section> -->
      <!--End paper poster -->



      <!--BibTex citation -->
      <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <div class="bibtex-header">
            <h2 class="title">BibTeX</h2>
            <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
              <i class="fas fa-copy"></i>
              <span class="copy-text">Copy</span>
            </button>
          </div>
          <pre id="bibtex-code"><code>@inproceedings{
lin2025AUR,
title={A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis},
author={Dongheng Lin, Mengxue Qu, Kunyang Han, Jianbo Jiao, Xiaojie Jin, Yunchao Wei},
booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
year={2025},
url={https://openreview.net/forum?id=Qla5PqFL0s}
}</code></pre>
        </div>
      </section>
      <!--End BibTex citation -->


      <footer class="footer">
        <div class="container">
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content">

                <p>
                  This page was built using the <a
                    href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic
                    Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io"
                    target="_blank">Nerfies</a> project page.
                  You are free to borrow the source code of this website, we just ask that you link back to this page in
                  the footer. <br> This website is licensed under a <a rel="license"
                    href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                </p>

              </div>
            </div>
          </div>
        </div>
      </footer>

      <!-- Statcounter tracking code -->

      <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

      <!-- End of Statcounter Code -->

</body>


</html>


